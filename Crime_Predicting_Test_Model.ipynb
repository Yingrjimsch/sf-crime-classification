{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8923e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seaborn import heatmap\n",
    "from sklearn import tree\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "#from preprocessing import preprocessing\n",
    "import preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "531ca704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aufruf py file\n",
    "\n",
    "Y_df, X_df = pp.preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4970c0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples:  700527\n",
      "test samples:  175132\n",
      "sample dimension:  26\n"
     ]
    }
   ],
   "source": [
    "#Train- data split into train and test datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_df, Y_df, train_size=0.8, random_state=0, stratify=Y_df)\n",
    "\n",
    "print(\"training samples: \", x_train.shape[0])\n",
    "print(\"test samples: \", x_test.shape[0])\n",
    "print(\"sample dimension: \", x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cbbe16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DayOfWeek', 'Weekend', 'Address', 'CoordinateClusters', 'year',\n",
       "       'month', 'quarter', 'hour', 'minute', 'NORTHERN', 'PARK', 'INGLESIDE',\n",
       "       'BAYVIEW', 'RICHMOND', 'CENTRAL', 'TARAVAL', 'TENDERLOIN', 'MISSION',\n",
       "       'SOUTHERN', 'temp', 'Partially cloudy', 'Clear',\n",
       "       'Rain, Partially cloudy', 'Overcast', 'Rain, Overcast', 'Rain'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1eb441c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train - x_train.mean())/ x_train.std() #immer Trainings Daten verwenden, auch bei den Testdaten\n",
    "\n",
    "x_test = (x_test - x_train.mean())/ x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08a9cad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 300)               8100      \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 150)               45150     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 150)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 300)               45300     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 39)                11739     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,289\n",
      "Trainable params: 110,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#220, 110, 0.2\n",
    "model.add(Input(shape=X_df.shape[1]))\n",
    "model.add(Dense(220, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(110, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(220, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(39, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d531841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38aaf673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "746459e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/100\n",
      "2803/2803 [==============================] - 19s 7ms/step - loss: 2.5216 - val_loss: 2.4624 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0009999000574917021.\n",
      "Epoch 2/100\n",
      "2803/2803 [==============================] - 19s 7ms/step - loss: 2.4680 - val_loss: 2.4325 - lr: 9.9990e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.000999700106714659.\n",
      "Epoch 3/100\n",
      "2803/2803 [==============================] - 18s 7ms/step - loss: 2.4470 - val_loss: 2.4132 - lr: 9.9970e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0009994003415259673.\n",
      "Epoch 4/100\n",
      "2803/2803 [==============================] - 19s 7ms/step - loss: 2.4305 - val_loss: 2.4030 - lr: 9.9940e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0009990007918579775.\n",
      "Epoch 5/100\n",
      "2803/2803 [==============================] - 20s 7ms/step - loss: 2.4218 - val_loss: 2.3959 - lr: 9.9900e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0009985014876310735.\n",
      "Epoch 6/100\n",
      "2803/2803 [==============================] - 20s 7ms/step - loss: 2.4154 - val_loss: 2.3901 - lr: 9.9850e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0009979026914447063.\n",
      "Epoch 7/100\n",
      "2803/2803 [==============================] - 20s 7ms/step - loss: 2.4114 - val_loss: 2.3891 - lr: 9.9790e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0009972046657933619.\n",
      "Epoch 8/100\n",
      "2803/2803 [==============================] - 20s 7ms/step - loss: 2.4078 - val_loss: 2.3866 - lr: 9.9720e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0009964075567443476.\n",
      "Epoch 9/100\n",
      "2803/2803 [==============================] - 20s 7ms/step - loss: 2.4044 - val_loss: 2.3840 - lr: 9.9641e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0009955116266172385.\n",
      "Epoch 10/100\n",
      "2803/2803 [==============================] - 20s 7ms/step - loss: 2.4007 - val_loss: 2.3781 - lr: 9.9551e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0009945171376267872.\n",
      "Epoch 11/100\n",
      "2803/2803 [==============================] - 25s 9ms/step - loss: 2.3978 - val_loss: 2.3751 - lr: 9.9452e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.000993424351882975.\n",
      "Epoch 12/100\n",
      "2803/2803 [==============================] - 25s 9ms/step - loss: 2.3953 - val_loss: 2.3744 - lr: 9.9342e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0009922336476668565.\n",
      "Epoch 13/100\n",
      "2803/2803 [==============================] - 27s 10ms/step - loss: 2.3934 - val_loss: 2.3730 - lr: 9.9223e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00099094540310837.\n",
      "Epoch 14/100\n",
      "2803/2803 [==============================] - 22s 8ms/step - loss: 2.3912 - val_loss: 2.3712 - lr: 9.9095e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0009895599961864132.\n",
      "Epoch 15/100\n",
      "2803/2803 [==============================] - 23s 8ms/step - loss: 2.3896 - val_loss: 2.3697 - lr: 9.8956e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0009880779209698787.\n",
      "Epoch 16/100\n",
      "2803/2803 [==============================] - 24s 8ms/step - loss: 2.3885 - val_loss: 2.3699 - lr: 9.8808e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0009864995551009932.\n",
      "Epoch 17/100\n",
      "2803/2803 [==============================] - 28s 10ms/step - loss: 2.3875 - val_loss: 2.3701 - lr: 9.8650e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0009848253922889341.\n",
      "Epoch 18/100\n",
      "2803/2803 [==============================] - 28s 10ms/step - loss: 2.3863 - val_loss: 2.3668 - lr: 9.8483e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0009830559260457499.\n",
      "Epoch 19/100\n",
      "2803/2803 [==============================] - 20s 7ms/step - loss: 2.3847 - val_loss: 2.3676 - lr: 9.8306e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0009811916496864585.\n",
      "Epoch 20/100\n",
      "2803/2803 [==============================] - 20s 7ms/step - loss: 2.3828 - val_loss: 2.3690 - lr: 9.8119e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.000979233172512102.\n",
      "Epoch 21/100\n",
      "2803/2803 [==============================] - 25s 9ms/step - loss: 2.3825 - val_loss: 2.3661 - lr: 9.7923e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0009771811035805126.\n",
      "Epoch 22/100\n",
      "2803/2803 [==============================] - 20s 7ms/step - loss: 2.3813 - val_loss: 2.3642 - lr: 9.7718e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0009750360517064341.\n",
      "Epoch 23/100\n",
      "2803/2803 [==============================] - 23s 8ms/step - loss: 2.3802 - val_loss: 2.3652 - lr: 9.7504e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0009727986254616435.\n",
      "Epoch 24/100\n",
      "2803/2803 [==============================] - 23s 8ms/step - loss: 2.3797 - val_loss: 2.3637 - lr: 9.7280e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0009704694912433684.\n",
      "Epoch 25/100\n",
      "2803/2803 [==============================] - 22s 8ms/step - loss: 2.3783 - val_loss: 2.3640 - lr: 9.7047e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0009680493732454474.\n",
      "Epoch 26/100\n",
      "2803/2803 [==============================] - 21s 7ms/step - loss: 2.3774 - val_loss: 2.3637 - lr: 9.6805e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0009655389953727925.\n",
      "Epoch 27/100\n",
      "2803/2803 [==============================] - 23s 8ms/step - loss: 2.3767 - val_loss: 2.3623 - lr: 9.6554e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0009629390812415337.\n",
      "Epoch 28/100\n",
      "2803/2803 [==============================] - 19s 7ms/step - loss: 2.3760 - val_loss: 2.3623 - lr: 9.6294e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0009602503541791623.\n",
      "Epoch 29/100\n",
      "2803/2803 [==============================] - 19s 7ms/step - loss: 2.3752 - val_loss: 2.3622 - lr: 9.6025e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0009574736533033693.\n",
      "Epoch 30/100\n",
      "2803/2803 [==============================] - 19s 7ms/step - loss: 2.3742 - val_loss: 2.3620 - lr: 9.5747e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.0009546098173972024.\n",
      "Epoch 31/100\n",
      "2803/2803 [==============================] - 23s 8ms/step - loss: 2.3735 - val_loss: 2.3611 - lr: 9.5461e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.0009516596849092325.\n",
      "Epoch 32/100\n",
      "2803/2803 [==============================] - 23s 8ms/step - loss: 2.3730 - val_loss: 2.3600 - lr: 9.5166e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.000948624093953722.\n",
      "Epoch 33/100\n",
      "2803/2803 [==============================] - 20s 7ms/step - loss: 2.3721 - val_loss: 2.3630 - lr: 9.4862e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0009455039403269966.\n",
      "Epoch 34/100\n",
      "2803/2803 [==============================] - 21s 8ms/step - loss: 2.3734 - val_loss: 2.3621 - lr: 9.4550e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.000942300119468278.\n",
      "Epoch 35/100\n",
      "2803/2803 [==============================] - 19s 7ms/step - loss: 2.3722 - val_loss: 2.3622 - lr: 9.4230e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.0009390135844645062.\n",
      "Epoch 36/100\n",
      "2803/2803 [==============================] - 18s 6ms/step - loss: 2.3710 - val_loss: 2.3605 - lr: 9.3901e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.0009356452880227539.\n",
      "Epoch 37/100\n",
      "2803/2803 [==============================] - 18s 7ms/step - loss: 2.3707 - val_loss: 2.3605 - lr: 9.3565e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.0009321961824704157.\n",
      "Epoch 38/100\n",
      "2803/2803 [==============================] - 19s 7ms/step - loss: 2.3701 - val_loss: 2.3604 - lr: 9.3220e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.0009286672197553972.\n",
      "Epoch 39/100\n",
      "2803/2803 [==============================] - 19s 7ms/step - loss: 2.3693 - val_loss: 2.3621 - lr: 9.2867e-04\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.0009250594674093705.\n",
      "Epoch 40/100\n",
      "1108/2803 [==========>...................] - ETA: 11s - loss: 2.3644"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-d39ae820bafb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdecay\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m     11\u001b[0m   \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "epochs = 100\n",
    "decay = initial_learning_rate / epochs\n",
    "\n",
    "def lr_time_based_decay(epoch, lr):\n",
    "    return lr * 1 / (1 + decay * epoch)\n",
    "\n",
    "history = model.fit(\n",
    "  x_train,\n",
    "  y_train,\n",
    "  batch_size=200,\n",
    "  epochs=100,\n",
    "  verbose=1,\n",
    "  validation_split=.2,\n",
    "  callbacks=[early_stopping, LearningRateScheduler(lr_time_based_decay, verbose=1)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "def plot_history(history: keras.callbacks.History):\n",
    "  \"\"\"\n",
    "  plot the training and validation loss for each training epoch\n",
    "\n",
    "  history: a History object, output of the .fit method of a keras model\n",
    "  \"\"\"\n",
    "  n = len(history.history['loss'])\n",
    "  plt.plot(np.arange(n), history.history['loss'], label=\"training loss\")\n",
    "  plt.plot(np.arange(n), history.history['val_loss'], label=\"validation loss\")\n",
    "  plt.xticks(range(0, n + 1, 2))\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14741f7",
   "metadata": {},
   "source": [
    "#### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=150, max_depth=8, random_state=0).fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b77f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = clf.predict_proba(x_test)\n",
    "test_log_loss = log_loss(y_test, y_preds)\n",
    "print(\"Testing Log Loss:\", test_log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b3a61",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1cae9d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', max_depth=100, n_estimators=150,\n",
       "                       random_state=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=100, random_state=0, class_weight=\"balanced\",n_estimators=150)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "#y_pred = clf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd5263f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Log Loss: 3.440130711844978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "y_preds = clf.predict_proba(x_test)\n",
    "test_log_loss = log_loss(y_test, y_preds)\n",
    "print(\"Testing Log Loss:\", test_log_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc2f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
