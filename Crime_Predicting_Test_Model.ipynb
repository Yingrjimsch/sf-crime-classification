{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8923e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seaborn import heatmap\n",
    "from sklearn import tree\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "#from preprocessing import preprocessing\n",
    "import preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531ca704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aufruf py file\n",
    "\n",
    "Y_df, X_df = pp.preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4970c0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples:  700527\n",
      "test samples:  175132\n",
      "sample dimension:  28\n"
     ]
    }
   ],
   "source": [
    "#Train- data split into train and test datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_df, Y_df, train_size=0.8, random_state=0, stratify=Y_df)\n",
    "\n",
    "print(\"training samples: \", x_train.shape[0])\n",
    "print(\"test samples: \", x_test.shape[0])\n",
    "print(\"sample dimension: \", x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb441c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train - x_train.mean())/ x_train.std() #immer Trainings Daten verwenden, auch bei den Testdaten\n",
    "\n",
    "x_test = (x_test - x_train.mean())/ x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a9cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#220, 110, 0.2\n",
    "model.add(Input(shape=X_df.shape[1]))\n",
    "model.add(Dense(220, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(110, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(210, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(39, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aaf673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746459e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "epochs = 100\n",
    "decay = initial_learning_rate / epochs\n",
    "\n",
    "def lr_time_based_decay(epoch, lr):\n",
    "    return lr * 1 / (1 + decay * epoch)\n",
    "\n",
    "history = model.fit(\n",
    "  x_train,\n",
    "  y_train,\n",
    "  batch_size=200,\n",
    "  epochs=100,\n",
    "  verbose=1,\n",
    "  validation_split=.2,\n",
    "  callbacks=[early_stopping, LearningRateScheduler(lr_time_based_decay, verbose=1)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "def plot_history(history: keras.callbacks.History):\n",
    "  \"\"\"\n",
    "  plot the training and validation loss for each training epoch\n",
    "\n",
    "  history: a History object, output of the .fit method of a keras model\n",
    "  \"\"\"\n",
    "  n = len(history.history['loss'])\n",
    "  plt.plot(np.arange(n), history.history['loss'], label=\"training loss\")\n",
    "  plt.plot(np.arange(n), history.history['val_loss'], label=\"validation loss\")\n",
    "  plt.xticks(range(0, n + 1, 2))\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
